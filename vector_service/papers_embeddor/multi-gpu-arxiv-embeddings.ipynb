{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T17:00:38.727567Z",
     "iopub.status.busy": "2021-10-26T17:00:38.727285Z",
     "iopub.status.idle": "2021-10-26T17:00:38.730682Z",
     "shell.execute_reply": "2021-10-26T17:00:38.730051Z",
     "shell.execute_reply.started": "2021-10-26T17:00:38.727541Z"
    }
   },
   "source": [
    "# arXiv Paper Embedding\n",
    "\n",
    "\n",
    "## Multi GPU w/ Dask + CUDF\n",
    "Using Dask and CuDF to orchestrate sentence embedding over multiple GPU workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Rapids and Dask Logos](https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/rapids_dask.png \"doc-image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Imports\n",
    "\n",
    "* [`dask_saturn`](https://github.com/saturncloud/dask-saturn) and [`dask_distributed`](http://distributed.dask.org/en/stable/): Set up and run the Dask cluster in Saturn Cloud.\n",
    "* [`dask-cudf`](https://docs.rapids.ai/api/cudf/stable/basics/dask-cudf.html): Create distributed `cudf` dataframes using Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import asyncio\n",
    "import dask_cudf\n",
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client, get_worker, wait\n",
    "import cudf\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import redis\n",
    "import re\n",
    "from redis.commands.search.field import VectorField, TagField\n",
    "\n",
    "from config.redis_config import ARXIV_PAPERS_PREFIX_KEY, INDEX_NAME, INDEX_TYPE, QUEUE_NAME, REDIS_URL\n",
    "from config import N_WORKERS, YEAR_PATTERN\n",
    "from lib.embeddings import Embeddings\n",
    "from lib.search_index import SearchIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_from_journal_ref(journal_ref: str) -> str:\n",
    "    if journal_ref:\n",
    "        years = [int(year) for year in re.findall(YEAR_PATTERN, journal_ref)]\n",
    "        year = str(min(years)) if years else \"\"\n",
    "    else:\n",
    "        year = \"\"\n",
    "    return year\n",
    "\n",
    "def process_categories(categories: str) -> str:\n",
    "    return \",\"\".join(categories.split(\" \"))\n",
    "\n",
    "\n",
    "def process_papers(papers:list[dict]) -> list[dict]:\n",
    "    embeddings = Embeddings()\n",
    "    return [\n",
    "        {\n",
    "            \"id\": paper[\"id\"],\n",
    "            \"year\": extract_year_from_journal_ref(paper[\"journal_ref\"]),\n",
    "            \"categories_processed\": process_categories(paper[\"categories\"]),\n",
    "            \"input\": embeddings.clean_description(paper[\"title\"] + \" \" + paper[\"abstract\"])\n",
    "        } for paper in papers\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_partition(df: dask_cudf.DataFrame):\n",
    "    \"\"\"\n",
    "    Create embeddings on single partition of DF (one dask worker)\n",
    "    \"\"\"\n",
    "    worker = get_worker()\n",
    "    if hasattr(worker, \"model\"):\n",
    "        model = worker.model\n",
    "    else:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "\n",
    "        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "        worker.model = model\n",
    "\n",
    "    print(\"embedding input\", flush=True)\n",
    "        \n",
    "    # embed the input      \n",
    "    vectors = model.encode(\n",
    "        sentences = df.input.values_host,\n",
    "        normalize_embeddings = True,\n",
    "        show_progress_bar = True\n",
    "    )\n",
    "    \n",
    "    # Convert to cudf series and return\n",
    "    df[\"vector\"] = cudf.Series(vectors.tolist(), index=df.index)\n",
    "    return df[[\"id\", \"vector\"]]\n",
    "\n",
    "def clear_workers():\n",
    "    \"\"\"\n",
    "    Deletes model attribute, freeing up memory on the Dask workers\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import gc\n",
    "\n",
    "    worker = get_worker()\n",
    "    if hasattr(worker, \"model\"):\n",
    "        del worker.model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arxiv_papers(redis_client, ids: list[str]) -> list[dict]:\n",
    "    pipe = redis_client.pipeline()\n",
    "    fields = [\"id\", \"title\", \"abstract\", \"categories\", \"journal_ref\"]\n",
    "    for id in ids:\n",
    "        pipe.hmget(f\"{ARXIV_PAPERS_PREFIX_KEY}/{id}\", fields)\n",
    "    papers_values = pipe.execute()\n",
    "    return [dict(zip(fields, values)) for values in papers_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding compute with dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_client = redis.from_url(REDIS_URL, decode_responses=True)\n",
    "\n",
    "cluster = SaturnCluster(n_workers=N_WORKERS)\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(n_workers=N_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute embedding and preprocessing by batch of 50000 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_NUMBER_OF_VECTORS = redis_client.llen(QUEUE_NAME)\n",
    "print(INITIAL_NUMBER_OF_VECTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    papers_ids = redis_client.lpop(QUEUE_NAME, 100000)\n",
    "    if not papers_ids:\n",
    "        print(\"Queue is empty\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"{len(papers_ids)} papers to process\")\n",
    "\n",
    "        papers = get_arxiv_papers(redis_client, papers_ids)\n",
    "        print(\"Papers have been retrieved from Redis\")\n",
    "\n",
    "        processed_papers = process_papers(papers)\n",
    "        cdf = cudf.DataFrame(processed_papers)\n",
    "        ddf = dask_cudf.from_cudf(cdf, npartitions=N_WORKERS)\n",
    "\n",
    "        output_df = ddf[[\"id\", \"input\"]].map_partitions(\n",
    "            func = embed_partition,\n",
    "            meta = {\n",
    "              \"id\": object,\n",
    "              \"vector\": cudf.ListDtype('float32')\n",
    "            }\n",
    "        )\n",
    "        full_ddf = ddf.merge(output_df)\n",
    "        full_ddf=full_ddf.compute().to_pandas()\n",
    "\n",
    "        print(\"Embedding and processing is done\")\n",
    "\n",
    "        pipe = redis_client.pipeline()\n",
    "        for _,row in full_ddf.iterrows():\n",
    "            pipe.hset(\n",
    "                f'{ARXIV_PAPERS_PREFIX_KEY}/{row[\"id\"]}',\n",
    "                mapping={\n",
    "                    \"year\":row[\"year\"],\n",
    "                    \"categories_processed\":row[\"categories_processed\"],\n",
    "                    \"vector\":np.array(row[\"vector\"],dtype=np.float32).tobytes()        \n",
    "                }\n",
    "            )\n",
    "        result = pipe.execute()\n",
    "        print(\"Embeddings and preprocessing uploaded to Redis\")\n",
    "\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_index(redis_conn, index_name, index_type, number_of_vectors):\n",
    "    search_index = SearchIndex()\n",
    "    categories_field = TagField(\"categories_processed\", separator=\"|\")\n",
    "    year_field = TagField(\"year\", separator=\"|\")\n",
    "    try:\n",
    "        result = await redis_conn.ft(index_name).info()\n",
    "        print(f\"Index {index_name} already exists\")\n",
    "    except redis.ResponseError as e:\n",
    "        print(e)\n",
    "        print(\"Creating vector search index\")\n",
    "        if index_type == \"HNSW\":\n",
    "            await search_index.create_hnsw(\n",
    "                categories_field,\n",
    "                year_field,\n",
    "                redis_conn=redis_conn,\n",
    "                number_of_vectors=number_of_vectors,\n",
    "                prefix=ARXIV_PAPERS_PREFIX_KEY,\n",
    "                distance_metric=\"IP\",\n",
    "            )\n",
    "        else:\n",
    "            await search_index.create_flat(\n",
    "                categories_field,\n",
    "                year_field,\n",
    "                redis_conn=redis_conn,\n",
    "                number_of_vectors=number_of_vectors,\n",
    "                prefix=ARXIV_PAPERS_PREFIX_KEY,\n",
    "                distance_metric=\"IP\",\n",
    "            )\n",
    "        print(\"Search index created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_async_conn = redis.asyncio.from_url(REDIS_URL)\n",
    "await create_index(\n",
    "        redis_async_conn,\n",
    "        INDEX_NAME,\n",
    "        INDEX_TYPE,\n",
    "        INITIAL_NUMBER_OF_VECTORS\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
